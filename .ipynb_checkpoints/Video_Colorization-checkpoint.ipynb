{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##                                           **B W Video Colorization**\n",
    "This model runs Image colorization model over a video frame by frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INPUT VIDEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO = \"rio_32.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL PATH AND OUTPUT PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "prototxt = \"./model/colorization_deploy_v2.prototxt\"\n",
    "model = \"./model/colorization_release_v2.caffemodel\"\n",
    "points = \"./model/pts_in_hull.npy\"\n",
    "video =  \"./input_video/\"+VIDEO\n",
    "width = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = cv2.VideoCapture(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load our serialized black and white colorizer model and cluster center points from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net = cv2.dnn.readNetFromCaffe(prototxt,model)\n",
    "pts = np.load(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add the cluster centers as 1x1 convolutions to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class8 = net.getLayerId(\"class8_ab\")\n",
    "conv8 = net.getLayerId(\"conv8_313_rh\")\n",
    "pts = pts.transpose().reshape(2, 313, 1, 1)\n",
    "net.getLayer(class8).blobs = [pts.astype(\"float32\")]\n",
    "net.getLayer(conv8).blobs = [np.full([1, 313], 2.606, dtype=\"float32\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop over frames from the video stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-556f7968ae21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# 'a' and 'b' channel values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblobFromImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# resize the predicted 'ab' volume to the same dimensions as our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "count = 0\n",
    "success = True\n",
    "while success:\n",
    "\t# grab the next frame and handle if we are reading from either\n",
    "\t# VideoCapture or VideoStream\n",
    "\tsuccess, frame = vs.read()\n",
    "\n",
    "\t# if we are viewing a video and we did not grab a frame then we\n",
    "\t# have reached the end of the video\n",
    "\tif frame is None:\n",
    "\t\tbreak\n",
    "\n",
    "\t# resize the input frame, scale the pixel intensities to the\n",
    "\t# range [0, 1], and then convert the frame from the BGR to Lab\n",
    "\t# color space\n",
    "\tframe = imutils.resize(frame, 500)\n",
    "\tframe = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\tframe = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "\tscaled = frame.astype(\"float32\") / 255.0\n",
    "\tlab = cv2.cvtColor(scaled, cv2.COLOR_RGB2LAB)\n",
    "\n",
    "\t# resize the Lab frame to 224x224 (the dimensions the colorization\n",
    "\t# network accepts), split channels, extract the 'L' channel, and\n",
    "\t# then perform mean centering\n",
    "\tresized = cv2.resize(lab, (224, 224))\n",
    "\tL = cv2.split(resized)[0]\n",
    "\tL -= 50\n",
    "\n",
    "\t# pass the L channel through the network which will *predict* the\n",
    "\t# 'a' and 'b' channel values\n",
    "\tnet.setInput(cv2.dnn.blobFromImage(L))\n",
    "\tab = net.forward()[0, :, :, :].transpose((1, 2, 0))\n",
    "\n",
    "\t# resize the predicted 'ab' volume to the same dimensions as our\n",
    "\t# input frame, then grab the 'L' channel from the *original* input\n",
    "\t# frame (not the resized one) and concatenate the original 'L'\n",
    "\t# channel with the predicted 'ab' channels\n",
    "\tab = cv2.resize(ab, (frame.shape[1], frame.shape[0]))\n",
    "\tL = cv2.split(lab)[0]\n",
    "\tcolorized = np.concatenate((L[:, :, np.newaxis], ab), axis=2)\n",
    "\n",
    "\t# convert the output frame from the Lab color space to RGB, clip\n",
    "\t# any values that fall outside the range [0, 1], and then convert\n",
    "\t# to an 8-bit unsigned integer ([0, 255] range)\n",
    "\tcolorized = cv2.cvtColor(colorized, cv2.COLOR_LAB2BGR)\n",
    "\tcolorized = np.clip(colorized, 0, 1)\n",
    "\tcolorized = (255 * colorized).astype(\"uint8\")\n",
    "\n",
    "\t# show the original and final colorized frames\n",
    "\tcv2.imshow(\"Original\", frame)\n",
    "\tcv2.imshow(\"Colorized\", colorized)\n",
    "    \n",
    "\tcv2.imwrite(\"./colorized_video_frames/frame%d.jpg\" % count, colorized)\n",
    "\tcount += 1\n",
    "\tkey = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "\t# if the `q` key was pressed, break from the loop\n",
    "\tif key == ord(\"q\"):\n",
    "\t\tbreak\n",
    "\n",
    "vs.release()\n",
    "\n",
    "# close any open windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_frames_to_video(pathIn, pathOut, fps):\n",
    "    frame_array = []\n",
    "    files = [f for f in os.listdir(pathIn) if isfile(join(pathIn, f))]\n",
    " \n",
    "    #for sorting the file names properly\n",
    "    files.sort(key = lambda x: int(x[5:-4]))\n",
    " \n",
    "    for i in range(len(files)):\n",
    "        filename=pathIn + files[i]\n",
    "        #reading each files\n",
    "        img = cv2.imread(filename)\n",
    "        height, width, layers = img.shape\n",
    "        size = (width,height)\n",
    "        print(filename)\n",
    "        #inserting the frames into an image array\n",
    "        frame_array.append(img)\n",
    " \n",
    "    out = cv2.VideoWriter(pathOut,cv2.VideoWriter_fourcc(*'MJPG'), fps, size)\n",
    " \n",
    "    for i in range(len(frame_array)):\n",
    "        # writing to a image array\n",
    "        out.write(frame_array[i])\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calling function to colorize the video and store output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathIn= './colorized_video_frames/'\n",
    "pathOut = './colorized_videos/video.avi'\n",
    "fps = 30.0\n",
    "convert_frames_to_video(pathIn, pathOut, fps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
